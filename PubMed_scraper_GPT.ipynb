{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d722b633",
   "metadata": {},
   "source": [
    "## GPT Article-Filter Version PubMed_scraper_GPT\n",
    "\n",
    "In this notebook, we utilize several libraries including `langchain`, `nltk`, `openai`, `pymed`, `Bio` among others, to filter articles based on specific criteria. \n",
    "\n",
    "The environment variable 'OPENAI_API_KEY' is set and the base API for `openai` is updated to \"https://fmops.ai/api/v1/proxy/openai/v1\".\n",
    "\n",
    "Below are the libraries used:\n",
    "\n",
    "1. `langchain` : Used for creating conversational AI models.\n",
    "\n",
    "2. `nltk` : Natural Language Toolkit, used for working with human language data.\n",
    "\n",
    "3. `openai` : Used to access the OpenAI API for generating human-like text.\n",
    "\n",
    "4. `os` : The OS module in Python provides functions for interacting with the operating system.\n",
    "\n",
    "5. `pymed` : Python wrapper for the PubMed Open Access database.\n",
    "\n",
    "6. `pandas` : A data manipulation and analysis library.\n",
    "\n",
    "7. `re` : Python's built-in module to work with Regular Expressions.\n",
    "\n",
    "8. `time` : This module provides various time-related functions.\n",
    "\n",
    "9. `requests` : Used for making HTTP requests in Python.\n",
    "\n",
    "10. `Bio` : Biopython is a set of freely available tools for biological computation.\n",
    "\n",
    "11. `docx` : Python library for creating and updating Microsoft Word (.docx) files.\n",
    "\n",
    "12. `spacy` : Library for advanced Natural Language Processing.\n",
    "\n",
    "13. `wordcloud` : A word cloud (or tag cloud) is a visual representation of text data.\n",
    "\n",
    "14. `docx.shared` : Allows sharing of certain common functions, classes and submodules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab881b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT Article-Filter Version\n",
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import openai\n",
    "import os\n",
    "from pymed import PubMed\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "from docx import Document\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "from docx.shared import Inches\n",
    "\n",
    "# Read API key from file\n",
    "api_key_file = \"/Users/ahmedumarbasha/Desktop/Github/apikey\"\n",
    "try:\n",
    "    with open(api_key_file, \"r\") as file:\n",
    "        openai_api_key = file.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: API key file not found at {api_key_file}\")\n",
    "    exit(1)\n",
    "\n",
    "# Print API key before setting\n",
    "# print(f\"API Key: {openai_api_key}\")\n",
    "\n",
    "# Set API key\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "# Manually download data to a specific folder\n",
    "# For \"punkt\"\n",
    "# Download from: https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip\n",
    "# Extract and place in /Users/ahmedumarbasha/Desktop/Github/PubMed_scraper_GPT/nltk_data/tokenizers/\n",
    "nltk.data.path.append('/Users/ahmedumarbasha/Desktop/Github/PubMed_scraper_GPT/nltk_data/tokenizers/')\n",
    "\n",
    "# For \"vader_lexicon\"\n",
    "# Download from: https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/sentiment/vader_lexicon.zip\n",
    "# Extract and place in /Users/ahmedumarbasha/Desktop/Github/PubMed_scraper_GPT/nltk_data/sentiment/\n",
    "nltk.data.path.append('/Users/ahmedumarbasha/Desktop/Github/PubMed_scraper_GPT/nltk_data/sentiment/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57123ec",
   "metadata": {},
   "source": [
    "## Creating Templates for Context and Gene\n",
    "\n",
    "In this part of the code, we create templates that will be used to provide a structured form of interaction with the AI model. The templates are designed in such a way that they define how a conversation or question should be structured.\n",
    "\n",
    "The **first template** is `template_Context`, it is used to ask the AI to explain what a certain gene is based on the provided text. The text is passed as the `abstract` variable and the gene is passed as the `gene` variable. \n",
    "\n",
    "The **second template** `template_Gene1` is used to ask the AI if a certain gene is used in the context of a full name gene or as a transcription factor based on a provided sentence.\n",
    "\n",
    "Each template is attached to a `PromptTemplate` which is then used by an `LLMChain` object to create a language model. The `LLMChain` uses `ChatOpenAI` model with a temperature of 0, which means the output will be deterministic and less random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe20bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_Context = \"\"\"<Question: Explain in detail what {gene} is in the Text Provided?><Text: {abstract}>\n",
    "Your Answer(Do not use abbreviation):\"\"\"\n",
    "\n",
    "prompt_Context = PromptTemplate(\n",
    "    input_variables=[\"abstract\", \"gene\"], \n",
    "    template=template_Context,\n",
    ")\n",
    "\n",
    "keyword_Context = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0), \n",
    "    prompt=prompt_Context\n",
    ")\n",
    "\n",
    "#-----------------------------\n",
    "template_Gene1 = \"\"\"\n",
    "<Is {gene} in the provided Text used in the context {fullName} gene or transcription factor? \n",
    "Say no if Text says not mentioned or does not appear)]>\n",
    "<Text: {sentence}>\n",
    "Your Answer(Yes or No only):\"\"\"\n",
    "\n",
    "prompt_Gene1 = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"gene\", \"fullName\"], \n",
    "    template=template_Gene1,\n",
    ")\n",
    "\n",
    "is_Gene = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0), \n",
    "    prompt=prompt_Gene1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c8d72",
   "metadata": {},
   "source": [
    "## Defining Supporting Functions\n",
    "\n",
    "A set of supporting functions are defined in this portion of the code. They serve various purposes, including search term generation, gene name fetching, information extraction, text cleaning, abstract fetching, and word cloud generation. These functions are critical in processing and transforming the data for further use.\n",
    "\n",
    "1. **gene_to_search**: This function generates search terms for a given gene and its full name. The search terms are constructed for PubMed search with a focus on Autism but excludes references to Cancer and Tumor.\n",
    "\n",
    "2. **gene_fullName**: This function retrieves the full name of a gene given its abbreviation by making a request to the 'mygene.info' API.\n",
    "\n",
    "3. **extract_geneInfo**: This function extracts the gene information from a given query string.\n",
    "\n",
    "4. **remove_html_tags**: This function removes HTML tags from a given text string.\n",
    "\n",
    "5. **fetch_abstract**: This function fetches the abstract of a paper from PubMed given its PMID. \n",
    "\n",
    "6. **generate_wordcloud**: This function generates a word cloud given an input text. The word cloud is based on gene names, entities of certain types, and certain patterns in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ee9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_to_search(element, fullName):\n",
    "    ls = []\n",
    "    ls.append(\"(\" + element + \"[Title/Abstract]) AND ((AUTISM[Title/Abstract]) OR (autistic[Title/Abstract])) \\\n",
    "    NOT (CANCER[Title/Abstract]) NOT (TUMOR[Title/Abstract])\")\n",
    "\n",
    "    ls.append(\"(\" + fullName + \"[Title/Abstract]) AND ((AUTISM[Title/Abstract]) OR (autistic[Title/Abstract])) \\\n",
    "    NOT (CANCER[Title/Abstract]) NOT (TUMOR[Title/Abstract])\")\n",
    "        \n",
    "    return ls\n",
    "\n",
    "def gene_fullName(gene_abbr):\n",
    "    url = f'https://mygene.info/v3/query?q=symbol:{gene_abbr}&fields=name'\n",
    "    time.sleep(0.1)\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    return data['hits'][0]['name']\n",
    "\n",
    "def extract_geneInfo(query):\n",
    "    split_query = query.split('(')\n",
    "    gene_info = split_query[1].split('[Title/Abstract]')\n",
    "    return gene_info[0]\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def fetch_abstract(pmid):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    try:\n",
    "        abstract_sections = records[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"]\n",
    "        abstract = \"\\n\".join(str(section) for section in abstract_sections)\n",
    "    except KeyError:\n",
    "        abstract = \"No abstract available\"\n",
    "    return abstract\n",
    "\n",
    "def generate_wordcloud(input_text, nlp1, nlp2, nlp3):\n",
    "    doc1 = nlp1(input_text)\n",
    "    doc2 = nlp2(input_text)\n",
    "    doc3 = nlp3(input_text)\n",
    "\n",
    "    gene_pattern = r\"^[A-Z]{1}[A-Za-z0-9_-]*[A-Za-z]{1}[A-Za-z0-9_-]*$\"\n",
    "    genes = [token.text for token in doc2 if re.match(gene_pattern, token.text) and token.pos_ == 'NOUN']\n",
    "    \n",
    "    entities1 = ['_'.join(ent.text.split()) for ent in doc1.ents if ent.label_ in \n",
    "                 {'ORGAN', 'CELL', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'PATHOLOGICAL_FORMATION'}]\n",
    "    entities2 = ['_'.join(ent.text.split()) for ent in doc2.ents if ent.label_ in \n",
    "                 {'DISEASE'}]\n",
    "    entities3 = [ent.text for ent in doc2.ents if ent.label_ in \n",
    "                 {'TAXON'}]\n",
    "    combined_entities = genes + entities1 + entities2 + entities3\n",
    "    \n",
    "    stem_cell_pattern = re.compile(r'\\bstem cell\\b', re.IGNORECASE)\n",
    "    ipsc_pattern = re.compile(r'\\bipsc\\b', re.IGNORECASE)\n",
    "    if re.search(stem_cell_pattern, input_text):\n",
    "        combined_entities.append('stem_cell')\n",
    "    if re.search(ipsc_pattern, input_text):\n",
    "        combined_entities.append('iPSC')\n",
    "\n",
    "    filtered_entities = []\n",
    "    for entity in combined_entities:\n",
    "        if not any([entity in other_entity and entity != other_entity for other_entity in combined_entities]):\n",
    "            filtered_entities.append(entity)\n",
    "\n",
    "    filtered_text = ' '.join(filtered_entities)\n",
    "\n",
    "    if not filtered_text:\n",
    "        img = Image.new('RGB', (400, 200), color='white')\n",
    "        return img\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white', max_words=100, contour_width=3, contour_color='steelblue')\n",
    "    wordcloud.generate(filtered_text)\n",
    "\n",
    "    return wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96ca95",
   "metadata": {},
   "source": [
    "## Defining Main Functions\n",
    "\n",
    "These main functions serve to search queries, analyze articles, and process extracted sentences. \n",
    "\n",
    "1. **search_Query_GPT**: This function uses a query to search PubMed for articles, fetches the articles' abstracts and other relevant information, applies filters, and stores the results in a DataFrame.\n",
    "\n",
    "2. **article_Interest**: This function analyzes the abstracts, predicts the context and gene information, and calculates an interest score using sentiment analysis.\n",
    "\n",
    "3. **extract_Sentences**: This function extracts sentences from the text containing a target keyword or an abbreviation in parentheses, and then processes them to remove unwanted details and redundancies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d465b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_Query_GPT(query, gene, fullName, keyword_Context_GPT, is_Gene_GPT):\n",
    "    Entrez.email = \"ahmed.u0022@gmail.com\"  \n",
    "    time.sleep(0.5)\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=5000)\n",
    "    record = Entrez.read(handle)\n",
    "    pmid_list = record[\"IdList\"]\n",
    "    article_list = []\n",
    "    element = extract_geneInfo(query)\n",
    "\n",
    "    for pmid in pmid_list:\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\")\n",
    "            time.sleep(1)\n",
    "            records = Entrez.read(handle)\n",
    "\n",
    "            try:\n",
    "                title = records[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"ArticleTitle\"]\n",
    "                title = remove_html_tags(title)\n",
    "                pub_date = records[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"Journal\"][\"JournalIssue\"][\"PubDate\"]\n",
    "                article_ids = records[\"PubmedArticle\"][0][\"PubmedData\"][\"ArticleIdList\"]\n",
    "                doi_url = \"NA\"\n",
    "                # The PubDate field can be a dictionary with 'Year', 'Month', and 'Day' keys, or just a 'Year' key\n",
    "                for article_id in article_ids:\n",
    "                    if article_id.attributes[\"IdType\"] == \"doi\":\n",
    "                        doi_url = \"https://doi.org/\" + article_id\n",
    "                if 'Year' in pub_date:\n",
    "                        year = pub_date['Year']\n",
    "                else:\n",
    "                    year = None\n",
    "                url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}\"\n",
    "                full_abstract = fetch_abstract(pmid)\n",
    "                full_abstract = remove_html_tags(full_abstract) if full_abstract else ''\n",
    "                title_and_abstract = title + full_abstract\n",
    "\n",
    "                #check if any lower cased homonyms are detected. \n",
    "                if element.isupper() and not re.search(r'\\d', element):\n",
    "                    if gene not in title_and_abstract:\n",
    "                        continue\n",
    "\n",
    "                    if fullName.lower() in title_and_abstract.lower() or (len(gene) >= 4 or re.search(r'\\d', gene)):\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(\"------------------------------------------\")\n",
    "                        print(url)\n",
    "                        temp = extract_Sentences(title_and_abstract, gene)\n",
    "                        time.sleep(1)\n",
    "                        score = article_Interest(gene, temp, fullName, keyword_Context_GPT,is_Gene_GPT)\n",
    "                        print('▶ ' + str(score))\n",
    "                        # Check if score is 1\n",
    "                        if score <= 0:\n",
    "                            continue\n",
    "\n",
    "                article_dict = {'info': \"Url: \" + url + \"\\n\" + \"DOI: \" + doi_url + \"\\n\\n\" + \"Title(\" + year + \"): \"\n",
    "                                + title + \"\\n\\n\" + full_abstract + \"\\n\\n\"}\n",
    "                article_list.append(article_dict)\n",
    "                \n",
    "\n",
    "            except IndexError:\n",
    "                print(f\"|Detected Excerpt, not an abstract, or no doi, with PMID:{pmid}|\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"|Detected Excerpt, not an abstract, or no doi, with PMID:{pmid}|\")\n",
    "            print(e)\n",
    "\n",
    "    search_df = pd.DataFrame(article_list)\n",
    "    return search_df\n",
    "\n",
    "def article_Interest(gene, full_Abstract, fullName, keyword_Context_GPT, is_Gene_GPT):\n",
    "    time.sleep(0.3)\n",
    "    AI_Context = keyword_Context_GPT.predict(abstract=full_Abstract, gene=gene)\n",
    "    time.sleep(0.3)\n",
    "    AI_Gene = is_Gene_GPT.predict(sentence=AI_Context, gene=gene, fullName=fullName)\n",
    "\n",
    "    print(AI_Context)\n",
    "    print('▶ ' + AI_Gene)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    article_Score = sid.polarity_scores(AI_Gene)\n",
    "    interest_Score = article_Score['compound']\n",
    "    \n",
    "    return 1 if interest_Score > 0 else 0\n",
    "\n",
    "\n",
    "def extract_Sentences(text, target_keyword):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    keyword_sentences = [sentence for sentence in sentences if target_keyword in sentence]\n",
    "    processed_sentences = []\n",
    "    abbreviation = []\n",
    "\n",
    "    for sentence in keyword_sentences:\n",
    "        if sentence.count(',') > 2:\n",
    "            temp = sentence.split(',')\n",
    "\n",
    "            keyword_index = -1\n",
    "            for i, part in enumerate(temp):\n",
    "                if target_keyword in part:\n",
    "                    keyword_index = i\n",
    "                    break\n",
    "\n",
    "            temp = [temp[0], temp[keyword_index], temp[-1]]\n",
    "\n",
    "            processed_sentences.append(','.join(temp))\n",
    "        else:\n",
    "            processed_sentences.append(sentence)\n",
    "\n",
    "    # Find sentences containing single-word abbreviations in parentheses\n",
    "    abbreviation_sentences = [sentence for sentence in sentences if re.search(r'\\([A-Za-z]+\\)', sentence)]\n",
    "\n",
    "    # Process abbreviation sentences\n",
    "    for sentence in abbreviation_sentences:\n",
    "        if sentence.count(',') > 2:\n",
    "            temp = sentence.split(',')\n",
    "\n",
    "            abbreviation_index = -1\n",
    "            for i, part in enumerate(temp):\n",
    "                if re.search(r'\\([A-Za-z]+\\)', part):\n",
    "                    abbreviation_index = i\n",
    "                    break\n",
    "\n",
    "            temp = [temp[0], temp[abbreviation_index], temp[-1]]\n",
    "\n",
    "            abbreviation.append(','.join(temp))\n",
    "        else:\n",
    "            abbreviation.append(sentence)\n",
    "\n",
    "    combined_text = ' '.join(abbreviation + processed_sentences)\n",
    "    # Remove words encapsulated in parentheses and any extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s\\([A-Za-z]+\\)', '', combined_text)\n",
    "    \n",
    "    # Remove abbreviations from the cleaned text\n",
    "    for abbr in abbreviation:\n",
    "        cleaned_text = cleaned_text.replace(abbr, '')\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2dd29",
   "metadata": {},
   "source": [
    "## Loading Spacy Models\n",
    "\n",
    "Here, we load three different models from SpaCy for named entity recognition. This step may take some time due to the size of the models.\n",
    "\n",
    "- `en_ner_bionlp13cg_md`: This model is trained on the BioNLP 13CG corpus and is suitable for recognizing various biomedical named entities.\n",
    "- `en_ner_bc5cdr_md`: This model is trained on the BC5CDR corpus, which focuses on recognizing chemical and disease named entities.\n",
    "- `en_ner_craft_md`: This model is trained on the CRAFT corpus, providing good performance for a broader range of biomedical named entities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b468ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/spacy/language.py:2141: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "# Provide the path to the model directory\n",
    "model_path1 = \"/Users/ahmedumarbasha/Desktop/Github/PubMed_scraper_GPT/spacy_models/en_ner_bionlp13cg_md-0.5.3/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.5.3\"\n",
    "model_path2 = \"/Users/ahmedumarbasha/Desktop/Github/PubMed_scraper_GPT/spacy_models/en_ner_bc5cdr_md-0.5.3/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.5.3\"\n",
    "model_path3 = \"/Users/ahmedumarbasha/Desktop/Github/PubMed_scraper_GPT/spacy_models/en_ner_craft_md-0.5.3/en_ner_craft_md/en_ner_craft_md-0.5.3\"\n",
    "\n",
    "# Load models from the specified paths\n",
    "nlp1 = spacy.load(model_path1)\n",
    "nlp2 = spacy.load(model_path2)\n",
    "nlp3 = spacy.load(model_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7b8db",
   "metadata": {},
   "source": [
    "## Setting up PubMed and Loading Gene List\n",
    "\n",
    "1. We first setup the `PubMed` tool by passing our tool name (`\"MyTool\"`) and an email address. Replace `\"choyoungb@gmail.com\"` with the email that you used to register on PubMed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df83f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KDM3A']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace email that you use for pubmed login\n",
    "pubmed = PubMed(tool=\"MyTool\", email=\"ahmed.u0022@gmail.com\")  # change to your email\n",
    "\n",
    "# Replace tf with your list of gene names\n",
    "original_file = pd.read_excel('TF2.xlsx')\n",
    "tf = original_file.iloc[:, 0].tolist()  # list of the first five genes\n",
    "print(tf)\n",
    "len(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93588e",
   "metadata": {},
   "source": [
    "## Data Processing and Document Generation\n",
    "\n",
    "The following steps are performed in this code block:\n",
    "\n",
    "1. **Initialize DataFrame**: We start by initializing an empty pandas DataFrame with columns 'gene' and 'info'.\n",
    "\n",
    "2. **Search and Process Genes**: For each gene in our transcription factors (tf) list, we conduct a search using the GPT model. The search results are stored in the DataFrame. If the gene does not exist in the 'gene' column of the DataFrame, we append it. Otherwise, we simply concatenate the new data with the existing DataFrame.\n",
    "\n",
    "3. **DataFrame Completion**: After processing all the genes and completing our DataFrame, we print a message indicating the completion of the DataFrame.\n",
    "\n",
    "4. **Document Initiation**: We then initiate a Word Document and add a table to it.\n",
    "\n",
    "5. **Word Cloud Generation and Insertion**: For each row in our DataFrame, we generate a word cloud from the 'info' column, save it as an image, and insert this image into our Word Document.\n",
    "\n",
    "6. **Document Saving**: Once we've processed all the rows in the DataFrame and inserted the corresponding word clouds, we save our Word Document and print a message indicating the completion of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabc056d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(KDM3A[Title/Abstract]) AND ((AUTISM[Title/Abstract]) OR (autistic[Title/Abstract]))     NOT (CANCER[Title/Abstract]) NOT (TUMOR[Title/Abstract])\n",
      "(lysine demethylase 3A[Title/Abstract]) AND ((AUTISM[Title/Abstract]) OR (autistic[Title/Abstract]))     NOT (CANCER[Title/Abstract]) NOT (TUMOR[Title/Abstract])\n",
      "DataFrame Complete\n",
      "docx Generated\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['gene', 'info'])\n",
    "\n",
    "\n",
    "for gene in tf:\n",
    "    fullName = re.sub(r'[^A-Za-z0-9\\s]+', ' ', gene_fullName(gene))\n",
    "    query_list = gene_to_search(gene, fullName)\n",
    "    \n",
    "    \n",
    "    for search_phrase in query_list:\n",
    "        print(search_phrase)\n",
    "        search_df = search_Query_GPT(search_phrase, gene, fullName, keyword_Context, is_Gene)\n",
    "        for index, row in search_df.iterrows():\n",
    "            existing_row = df[df['info'] == row['info']]\n",
    "\n",
    "            if not existing_row.empty:\n",
    "                if gene not in existing_row['gene'].values[0]:\n",
    "                    df.loc[existing_row.index, 'gene'] += f\", {gene}\"\n",
    "            else:\n",
    "                row['gene'] = gene\n",
    "                df = pd.concat([df, row.to_frame().T], ignore_index=True)       \n",
    "                \n",
    "print(\"DataFrame Complete\")\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "table = doc.add_table(rows=2 * len(df), cols=1)\n",
    "\n",
    "row_idx = 0\n",
    "for index, row in df.iterrows():\n",
    "    table.cell(row_idx, 0).text = str(row[\"gene\"])\n",
    "    row_idx += 1\n",
    "    abstract = row[\"info\"].split(\"Title: \")[-1].split(\"\\n\\n\", 1)[1]\n",
    "\n",
    "    wordcloud = generate_wordcloud(abstract, nlp1, nlp2, nlp3)\n",
    "\n",
    "    img_path = f\"wordcloud_{index}.png\"\n",
    "    if isinstance(wordcloud, WordCloud):\n",
    "        wordcloud.to_file(img_path)\n",
    "    else: \n",
    "        wordcloud.save(img_path)\n",
    "\n",
    "\n",
    "    table.cell(row_idx, 0).text = str(row[\"info\"])\n",
    "    paragraph = table.cell(row_idx, 0).paragraphs[0]\n",
    "    run = paragraph.add_run()\n",
    "    run.add_picture(img_path, width=Inches(6))\n",
    "\n",
    "\n",
    "    os.remove(img_path)\n",
    "\n",
    "    row_idx += 1\n",
    "\n",
    "doc.save(\"output.docx\")\n",
    "print(\"docx Generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6392c5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gene                                               info\n",
      "0  KDM3A  Url: https://pubmed.ncbi.nlm.nih.gov/37553546\\...\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f684ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960564ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81105ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cb570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc616e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9b38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590ceaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cf6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176971b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
